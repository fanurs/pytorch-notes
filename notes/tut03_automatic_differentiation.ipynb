{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHla9GIxbKIN/LjgSOobQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanurs/pytorch-notes/blob/main/notes/tut03_automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation"
      ],
      "metadata": {
        "id": "dxzQ6EMfx0Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are at least three ways to evaluate the derivative of a function: symbolic differentiation, numerical differentiation, and automatic differentiation.\n",
        "\n",
        "Symbolic differentiation is what we usually first learned in calculus. However, for most applications, it is difficult for computer to automate this process.\n",
        "\n",
        "Numerical differentiation uses the method of finite differences to approximate a derivative,\n",
        "$$ \\frac{df(x)}{dx} \\approx \\frac{f(x + \\delta x) - f(x)}{\\delta x} \\ . $$\n",
        "But this approach introduces round-off errors.\n",
        "\n",
        "Automatic differentiation (AD) exploits the fact that every function can be computed using a chain of arithmetic operations. Hence, using the chain rule, it is possible to evaluate the resulting derivative."
      ],
      "metadata": {
        "id": "7bNFu5rGx56w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AD is usually faster and numerically more stable."
      ],
      "metadata": {
        "id": "4fQNa7Kk0-Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any machine learning algorithm is essentially some kind of optimization problem. This is why having a reliable way to compute a gradient is very useful. PyTorch offers an automatic differeniation package called [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html) to handle these tasks."
      ],
      "metadata": {
        "id": "DTtKnF1Q1ykn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiating a single-variable function"
      ],
      "metadata": {
        "id": "-pEInF-u45C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch as th"
      ],
      "metadata": {
        "id": "qNkWEIcq5xnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the tensors"
      ],
      "metadata": {
        "id": "OuhHazm1_wya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by using PyTorch to solve a simple example. Given $g(x) = x^2$, evaluate\n",
        "$$ \\frac{d}{dx}g(x) $$\n",
        "over the domain $x\\in[-3, 3]$."
      ],
      "metadata": {
        "id": "Dvj5FG855Yww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.linspace(-3, 3, 301)\n",
        "y = x**2\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cRL_MsvX55La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far there is no differentiation involved. We are just plotting out the function $g(x) = x^2$ over the interval $x\\in[-3, 3]$ with 301-point sampling:\n",
        "$$ -3.00, -2.98, -2.96, \\ldots, 2.98, 3.00 \\ . $$\n",
        "\n",
        "To take the derivative, we would have to update an attribute, `torch.Tensor.requires_grad` to `True`. To propagate this attribute, we shall recompute `y` too."
      ],
      "metadata": {
        "id": "6WLbVENv6UtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad = True\n",
        "y = x**2\n",
        "x[0]"
      ],
      "metadata": {
        "id": "vwCJhC9R7Naa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that each entry of `x` is now a tensor that also contains the `grad_fn` property. This is where `autograd` will do its magic. But for now, it means if we try to plot by typing `plot.plot(x, y)`, an error will be raised. For `plot()` to work properly, we have to strip off the `grad_fn` part from the tensors. PyTorch provides us a `detach()` function for that."
      ],
      "metadata": {
        "id": "xBJKTWzO7hqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x.detach(), y.detach())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_hwca6gy9p0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking derivative as gradient"
      ],
      "metadata": {
        "id": "wooSxXWx_1CC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how can we actually take the derivative of $g(x)$?\n",
        "\n",
        "PyTorch's autograd can only compute the gradient,\n",
        "$$ \\nabla f(x_1, x_2, \\ldots, x_n) =\n",
        "\\begin{bmatrix}\n",
        "\\partial_1 f \\\\\n",
        "\\partial_2 f \\\\\n",
        "\\vdots \\\\\n",
        "\\partial_n f \\\\\n",
        "\\end{bmatrix} \\ .\n",
        "$$\n",
        "\n",
        "On the other hand, we have $g(x)$ evaluated at 301 discrete points,\n",
        "$$ x_k = -3 + 0.02 * (k - 1) $$\n",
        "for all $k = 1, 2, \\ldots, 301$. And we want to know its derivative $g'(x)$ at the same 301 discrete points,\n",
        "$$ g'(x_1), g'(x_2), \\ldots, g'(x_{301}) \\ . $$"
      ],
      "metadata": {
        "id": "E7YJc-nf_5o0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trick is to construct the sum\n",
        "$$ f(x_1, x_2, \\ldots, x_{301}) \\equiv g(x_1) + g(x_2) + \\cdots + g(x_{301}) \\ . $$\n",
        "Then the gradient of this sum would give us\n",
        "$$ \\nabla f(x_1, x_2, \\ldots, x_{301}) =\n",
        "\\begin{bmatrix}\n",
        "\\partial_1 f \\\\\n",
        "\\partial_2 f \\\\\n",
        "\\vdots \\\\\n",
        "\\partial_{301} f \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "g'(x_1) \\\\\n",
        "g'(x_2) \\\\\n",
        "\\vdots \\\\\n",
        "g'(x_{301}) \\\\\n",
        "\\end{bmatrix} \\ .\n",
        "$$\n",
        "\n",
        "In other words, instead of treating $g(x)$ as a single-variable function that depends on $x$, we may view $x_1, x_2, \\ldots, x_{301}$ as independent variables for the sum $f$. This is why constructing the tensors, we specified `requires_grad = True` for $x$, too."
      ],
      "metadata": {
        "id": "GGqkxvrlCvKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the trick, we first compute the sum of all function values evaluated at the 301 discrete points:"
      ],
      "metadata": {
        "id": "hyk14JbeEBy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = y.sum()\n",
        "total"
      ],
      "metadata": {
        "id": "p42I9Rio93HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we calculate the gradient. The function for this is `torch.Tensor.backward()`. The reason for this weird name is because computation of a gradient in a neural network often happens in a process called \"backward propagation\"."
      ],
      "metadata": {
        "id": "Mg_VDTGTD6bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total.backward()"
      ],
      "metadata": {
        "id": "Zw81NN299wN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `backward()` function does not actually return anything. Instead, it traces back all the tensors that were used to build up `total` (i.e. `x` and `y`), and takes the partial derivatives with respective to each component that has a `grad_fn` property (traced computation history) to construct the gradient of `total`. The final result will be stored as `grad` attribute:"
      ],
      "metadata": {
        "id": "nFf5bt9iF3R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x.detach(), x.grad.detach())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-T54C0D-E6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a line for $2x$, which is what we would expect for the derivative of $g(x) = x^2$."
      ],
      "metadata": {
        "id": "OVXqfADLGe1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us quickly apply all these steps to some more interesting function, $h(x) = \\sin(x^2)$:"
      ],
      "metadata": {
        "id": "hXFiYvG0G0HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.linspace(-3, 3, 100, requires_grad=True)\n",
        "y = th.sin(x**2)\n",
        "y.sum().backward()\n",
        "plt.plot(x.detach(), y.detach(), label='function')\n",
        "plt.plot(x.detach(), x.grad.detach(), label='derivative')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ff6DvOTZHPiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing automatic differentiation (AD) and finite difference method (FDM)"
      ],
      "metadata": {
        "id": "Y8sjcpMPJ7rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a function,\n",
        "$$ f(x) = \\sin(3x^2) \\ . $$\n",
        "We are interested in the derivative $f'(x)$ over the interval $x \\in [0, 2\\pi]$."
      ],
      "metadata": {
        "id": "hxCWpw8hJ-HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "    return th.sin(3 * x**2)"
      ],
      "metadata": {
        "id": "B7aiVbES7Rsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finite difference method (FDM)"
      ],
      "metadata": {
        "id": "pSHs87E30t7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using FDM, the number of grid points directly affects the accuracy of our derivatives."
      ],
      "metadata": {
        "id": "oOxkeSHb1Rmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_100 = th.linspace(0, th.pi, 100)\n",
        "x_20 = th.linspace(0, th.pi, 20)\n",
        "f_100 = model(x_100)\n",
        "f_20 = model(x_20)\n",
        "\n",
        "x = th.linspace(0, th.pi, 1000)\n",
        "plt.title(r'$f(x)$')\n",
        "plt.plot(x, model(x), color='green', label='actual')\n",
        "plt.scatter(x_100,f_100, marker='o', s=5, color='red', label='100')\n",
        "plt.scatter(x_20, f_20, marker='s', s=10, color='blue', label='20')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bZijumrB1sbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The derivatives can be estimated using the central difference:\n",
        "$$ \\frac{d}{dx}f(x_i) \\approx \\frac{f(x_{i+1}) - f(x_{i-1})}{2\\delta x} $$"
      ],
      "metadata": {
        "id": "XpMIJvNO21xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deriv_100 = 0.5 * (f_100[2:] - f_100[:-2]) / (x_100[1] - x_100[0])\n",
        "deriv_20 = 0.5 * (f_20[2:] - f_20[:-2]) / (x_20[1] - x_20[0])\n",
        "\n",
        "x = th.linspace(0, th.pi, 1000)\n",
        "plt.title(r\"$f'(x)$\")\n",
        "plt.plot(x, 6 * x * th.cos(3 * x**2), color='green', label='actual')\n",
        "plt.scatter(x_100[1:-1], deriv_100, marker='o', s=5, color='red', label='100')\n",
        "plt.scatter(x_20[1:-1], deriv_20, marker='s', s=10, color='blue', label='20', zorder=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SMtwWeUM4Fce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this plot, we have compared both numerical results against the symbolic result of\n",
        "$$ \\frac{d}{dx}f(x) = 6x\\cos(3x^2) \\ . $$\n",
        "As we can see, the accuracy for 20-point grid is quite terrible."
      ],
      "metadata": {
        "id": "R8Moykv76NFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation (AD)"
      ],
      "metadata": {
        "id": "GLL8Ok-P6o5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how AD would perform on both 100-point and 20-point grids."
      ],
      "metadata": {
        "id": "xDyEUWcA7By1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_100.requires_grad = True\n",
        "x_20.requires_grad = True\n",
        "model(x_100).sum().backward()\n",
        "model(x_20).sum().backward()\n",
        "\n",
        "x = th.linspace(0, th.pi, 1000)\n",
        "plt.title(r\"$f'(x)$\")\n",
        "plt.plot(x, 6 * x * th.cos(3 * x**2), color='green', label='actual')\n",
        "plt.scatter(x_100.detach(), x_100.grad.detach(), marker='o', s=5, color='red', label='100')\n",
        "plt.scatter(x_20.detach(), x_20.grad.detach(), marker='s', s=10, color='blue', label='20', zorder=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2rdhJj67Hq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the accuracy of AD is independent of the number of grid points."
      ],
      "metadata": {
        "id": "cgZTyGfG8p1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remarks"
      ],
      "metadata": {
        "id": "eKjxZYKI-PlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be cautious! We are not trying to tell you that FDM is always inaccurate. As long as we have access to the evaluation of $f(x)$, e.g. $f(x)$ is not too expensive to compute, we can simply evaluate the derivative according to\n",
        "$$ \\frac{d}{dx}f(x_i) \\approx \\frac{f(x_i + \\delta x) - f(x_i - \\delta x)}{\\delta x} \\ , $$\n",
        "where $\\delta x > 0$ can be made as small as we want until the estimated derivative attains the desired precision."
      ],
      "metadata": {
        "id": "y5sdFE75-O33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real challenge for FDM as opposed to AD is that, how do we know what $\\delta x$ to use? A common practice is to check the convergence iteratively. More specifically, one can use $\\delta x = 0.1, 0.01, 0.001, \\ldots$ iteratively, until the estimated derivatives converge within some tolerance. But this is very inefficient. AD, on the other hand, can \"automatically\" figure out the derivatives accurately."
      ],
      "metadata": {
        "id": "mVeY7x2xBvLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, AD also has its limitation. AD calculates the derivative using the chain rule for each primitive function that builds up the final functional expression. This means that when implementing $f(x)$, we are limited to using a finite set of functions to construct $f(x)$. In some cases, $f(x)$ might not have a closed form solution, and that would make it very difficult or simply impossible to use AD anymore. AD cannot be used for functions that are constructed empirically, for example, if $f(t)$ represents the live temperature of a town as a function of time $t$, then it is unlikely for us to precisely construct $f(t)$ using elementary functions (though approximation or interpolation may be good enough)."
      ],
      "metadata": {
        "id": "OJECCgrjBtgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch has provided users a rather long list of functions that can be used to with AD (see https://pytorch.org/docs/stable/torch.html). But if there is a function that cannot be constructed from all those listed functions, then you are out of luck. The following code cell, we show that feeding a tensor with `requires_grad = True` into non-PyTorch functions (e.g. NumPy functions) will give a `RuntimeError`."
      ],
      "metadata": {
        "id": "ZOfX0dhiId7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def non_torch_model(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "x = th.linspace(0, th.pi, 5)\n",
        "display(non_torch_model(x)) # no error\n",
        "\n",
        "x.requires_grad = True\n",
        "display(non_torch_model(x)) # RuntimeError"
      ],
      "metadata": {
        "id": "vMFreZv4IJ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "5ejCvL22x2yX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://en.wikipedia.org/wiki/Automatic_differentiation"
      ],
      "metadata": {
        "id": "pYs8s9Rkx39C"
      }
    }
  ]
}